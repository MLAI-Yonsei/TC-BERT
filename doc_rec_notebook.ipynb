{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from typing import Union, List, Dict\n",
    "from collections import Counter, defaultdict\n",
    "import argparse\n",
    "import pdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running option\n",
    "class Config():\n",
    "    seed = 0\n",
    "    model = 'best'\n",
    "    run_scratch = 0\n",
    "    src_id = 0\n",
    "    topk = 3\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path: str = './data/etri4rec.csv'\n",
    "vocab_path: str = './vocab'\n",
    "result_path: str = './result'\n",
    "\n",
    "last_model: str = './model/checkpoint-12300/'\n",
    "best_model: str = './model/checkpoint-984/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproduction\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build data dicitionary ...\n",
      "# of documents: 1078\n"
     ]
    }
   ],
   "source": [
    "# build dataset (dictionary type => id : [sent1, sent2, ..., ])\n",
    "# and nested dictionary for analysis on recommendation result\n",
    "# {document id : {sentence id : text}} \n",
    "print(\"build data dicitionary ...\")\n",
    "df = pd.read_csv(data_path, encoding='cp949')\n",
    "original_doc_idx = df.doc_id.unique()\n",
    "tran2ori = {k:v for k, v in enumerate(original_doc_idx)}\n",
    "print(f\"# of documents: {len(original_doc_idx)}\")\n",
    "\n",
    "data_dict = {}\n",
    "nested_data_dict_t = defaultdict(lambda: defaultdict(int))\n",
    "for i, doc in enumerate(original_doc_idx):\n",
    "    data_dict[i] = list(df[df.doc_id == doc].text)\n",
    "    for j, sent in enumerate(np.where(df.doc_id == doc)[0]):\n",
    "        nested_data_dict_t[i][j] = df.iloc[sent].text\n",
    "nested_data_dict = {k: {k2: v2 for k2, v2 in v.items()} for k, v in nested_data_dict_t.items()}\n",
    "\n",
    "# label list and label-to-id dictionary\n",
    "# possible_labels = ['가설 설정', '기술 정의', '기술동향','기술의 파급효과', \n",
    "#                    '기술의 필요성', '대상 데이터', '데이터처리', '문제 정의', \n",
    "#                    '성능/효과', '시장동향', '이론/모형', '제안 방법', '후속연구']\n",
    "possible_labels = ['성능/효과', '제안 방법', '대상 데이터', '문제 정의', \n",
    "                   '이론/모형', '후속연구', '기술 정의','데이터처리', '가설 설정', \n",
    "                   '시장동향', '기술의 파급효과', '기술동향', '기술의 필요성']\n",
    "\n",
    "label_dict = {}\n",
    "label2id = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "    label2id[index] = possible_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# intialize tokenizer and load pre-trained TechBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained(vocab_path, do_lower_case=False, model_max_length=128)\n",
    "model_path = best_model if args.model == 'best' else last_model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, \n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(object):\n",
    "    def __init__(self, model, tokenizer, dataset) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ds = dataset\n",
    "        self.n_docs = len(dataset)\n",
    "        self.pool_emb = {}\n",
    "        self.pool_logits = {}\n",
    "        self.pool_pred = {}\n",
    "    \n",
    "    def encode_src(self, src_id):\n",
    "        '''\n",
    "            source 문서 하나를 받아 인코딩하고 관련정보 저장\n",
    "        '''\n",
    "        # raise error when the inputed id is out of the docs range\n",
    "        if src_id >= self.n_docs:\n",
    "            raise ValueError(f'src_id must be less than {self.n_docs}')\n",
    "            \n",
    "        inputs = tokenizer(self.ds[src_id], add_special_tokens=True, padding='max_length', return_tensors=\"pt\")\n",
    "        #pdb.set_trace()\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            logits, emb = out[0], out[1][12][:,0,:].squeeze()\n",
    "            # (# of sentence, # of categories), (# of sentence, emb dim)\n",
    "            # (N, C), (N, D)\n",
    "            predicted_class_id = logits.argmax(dim=1).numpy()\n",
    "            prediction = [label2id[i] for i in predicted_class_id]\n",
    "\n",
    "        # source document info.\n",
    "        self.src_id = src_id\n",
    "        self.src_logits = logits\n",
    "        self.src_emb = emb\n",
    "        self.src_pred = prediction\n",
    "    \n",
    "    def encode_pool(self):\n",
    "        '''\n",
    "            전체 데이터셋을 인코딩하고 딕셔너리 형태로 정보저장 \n",
    "            {id: emb}, {id: logit}, {id: pred}\n",
    "        '''\n",
    "        for id in range(self.n_docs):\n",
    "            inputs = tokenizer(self.ds[id], add_special_tokens=True, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                out = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "                logits, emb = out[0], out[1][12][:,0,:].squeeze()\n",
    "                # (# of sentence, # of categories), (# of sentence, emb dim)\n",
    "                # (N, C), (N, D)\n",
    "                predicted_class_id = logits.argmax(dim=1).numpy()\n",
    "                prediction = [label2id[i] for i in predicted_class_id]\n",
    "            \n",
    "            # build document pool\n",
    "            self.pool_emb[id] = emb\n",
    "            self.pool_logits[id] = logits\n",
    "            self.pool_pred[id] = prediction\n",
    "    \n",
    "    def src_checker(self):\n",
    "        '''\n",
    "            source 문서에 포함된 문장들에 대해 모델이 예측한 태그 반환\n",
    "        '''\n",
    "        src_tag_dic = Counter(self.src_pred)\n",
    "        return list(src_tag_dic.keys())\n",
    "\n",
    "    def recommendation(self, tag, src_id=None, topk=1):\n",
    "        '''\n",
    "            주어진 src 문서의 tag 문장들에 대한 TechBERT의 평균 임베딩을 기준으로,\n",
    "            pool 문서들 중 tag 문장들의 TechBERT 평균 임베딩이 가장 유사한 문서들의 id를 반환\n",
    "            + 거기에 더하여, 왜 그런 추천을 했는지 해석하기 위해 src 문서와 추천된 문서들에서 \n",
    "              해당 tag로 예측된 문장들 인덱스도 함께 반환\n",
    "        '''\n",
    "        if src_id is not None:\n",
    "            self.encode_src(src_id)\n",
    "        \n",
    "        # 1. get query sentence indicies and make query vector\n",
    "        src_tag_idx = [i for i, pred in enumerate(self.src_pred) if pred == tag]\n",
    "        query_embs = self.src_emb[src_tag_idx]\n",
    "        query_emb = query_embs.mean(dim=0).unsqueeze(0) #! (1, D)\n",
    "\n",
    "        # 2. build embedding matrix w.r.t. query tag\n",
    "        key_embs = torch.zeros(self.n_docs, query_emb.shape[1]) #! (N, D)\n",
    "        for id in range(self.n_docs):\n",
    "            tag_idx_i = [i for i, pred in enumerate(self.pool_pred[id]) if pred == tag]\n",
    "            if len(tag_idx_i) != 0:\n",
    "                key_embs_i = self.pool_emb[id][tag_idx_i]\n",
    "                key_embs[id] = key_embs_i.mean(dim=0).unsqueeze(0)\n",
    "            else:\n",
    "                key_embs[id] = 0 #! doc has no prediction as query tag\n",
    "        \n",
    "        # 3. embedding-similarity-based topk target documents\n",
    "        sim_matrix = self.similarity(query_emb, key_embs) #! (1, N)\n",
    "        sim_matrix[0, self.src_id] = -np.inf #! filter the source document\n",
    "        topk_idx = torch.topk(sim_matrix, k=topk)[1].squeeze()\n",
    "\n",
    "        # 4. aggregate target indices\n",
    "        tar_tag_idx = [ [i for i, pred in enumerate(self.pool_pred[id]) if pred == tag] for id in np.array(topk_idx)]\n",
    "        return topk_idx, src_tag_idx, tar_tag_idx\n",
    "    \n",
    "    # define similarity measure\n",
    "    def similarity(self, query, key, strategy='cos'):\n",
    "        if strategy == 'cos':\n",
    "            query      = torch.nn.functional.normalize(query, dim=1)\n",
    "            key        = torch.nn.functional.normalize(key, dim=1)\n",
    "            sim_matrix = query @ key.T\n",
    "        else:\n",
    "            raise ValueError('not implemented')\n",
    "        return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.run_scratch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load recommender ...\n"
     ]
    }
   ],
   "source": [
    "# initialize recommender system\n",
    "rec_sys = Recommender(model, tokenizer, data_dict)\n",
    "\n",
    "# build embedding pool\n",
    "if not args.run_scratch:\n",
    "    #! load extracted pool embedding \n",
    "    #! 사전에 저장해놓은 recommender 클래스 및 문서 임베딩 풀 로드\n",
    "    print('load recommender ...')\n",
    "    rec_sys = pickle.load(open('./techbert_rec.pkl', 'rb'))\n",
    "else:\n",
    "    #! initially extract pool embedding\n",
    "    #! TechBERT를 통해 전체 문서들을 encode\n",
    "    print('encode entire docs ...')\n",
    "    rec_sys.encode_pool()\n",
    "\n",
    "    print('save recommender ...')\n",
    "    pickle.dump(rec_sys, open('./techbert_rec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_result(src_id, topk, result_path):\n",
    "    #! 결과 저장 파일 경로\n",
    "    if os.path.isdir(result_path): pass\n",
    "    else: os.mkdir(result_path)\n",
    "    file_path = os.path.join(result_path, f'rec_for_{src_id}.txt')\n",
    "    if os.path.exists(file_path): os.remove(file_path)\n",
    "    out = open(file_path, 'a')\n",
    "    \n",
    "    #! TechBERT을 통한 소스문서 인코딩\n",
    "    print(f'encode source document (id: {src_id})', file=out)\n",
    "    rec_sys.encode_src(src_id = src_id)\n",
    "\n",
    "    #! 추천 소스 문서의 각 문장들에 대해 모델이 예측한 태그들을 반환\n",
    "    print(f'check tags in source doc {src_id} (tran) {tran2ori[src_id]} (ori) ', file=out)\n",
    "    src_tags = rec_sys.src_checker()\n",
    "    print(src_tags, file=out)\n",
    "\n",
    "    for search_tag in src_tags:\n",
    "        #! 소스 문서에 대해 확보된 각 태그들에 대하여 전체 대상문서 풀로부터,\n",
    "        #! 각 태그 기준으로 가장 유사한 topK 타겟 문서들의 인덱스와, \n",
    "        #! 결과 해석을 위해 소스/타겟 문서들 내에서 해당 태그로 분류된 모든 문장들의 인덱스 및 실제 문장을 반환\n",
    "        print('\\n=====================================================', file=out)\n",
    "        print(f'recommend target docs that has similar \"{search_tag}\" semantic with source doc', file=out)\n",
    "        rec_docs, src_idx, tar_idx = rec_sys.recommendation(tag=search_tag, topk=topk)\n",
    "        print(f\"recommended TOP-{topk} docs: {rec_docs} (tran), {[tran2ori[i] for i in rec_docs.tolist()]} (ori)\", file=out)\n",
    "        print(f\"(source) predicted as {search_tag}: {src_idx}\", file=out)\n",
    "        print(\"(source) query sentences:\", file=out)\n",
    "        for idx in src_idx:\n",
    "            print(nested_data_dict[src_id][idx], file=out)\n",
    "\n",
    "        print(f\"\\n(target) predicted as {search_tag}: {tar_idx}\", file=out)\n",
    "        for i, tar_doc in enumerate(rec_docs.tolist()):\n",
    "            print(f\"(target doc {tar_doc}) key sentences:\", file=out)\n",
    "            for sent_id in tar_idx[i]:\n",
    "                print(nested_data_dict[tar_doc][sent_id], file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot err cnt: 0\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "cnt = 0\n",
    "for id in range(0, 30, 5):\n",
    "    try:\n",
    "        rec_result(id, args.topk, result_path=result_path)\n",
    "    except:\n",
    "        cnt += 1\n",
    "        print(f'Error is raised during processing doc {id}')\n",
    "print(f'tot err cnt: {cnt}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('chang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b65f0a2caab02b8c00c2e9fc952daf29a52e22b10726b8d60fc07eefcd642fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
